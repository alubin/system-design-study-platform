export interface QuizQuestion {
  id: string
  type: 'multiple-choice' | 'true-false' | 'scenario'
  category: string
  difficulty: 'easy' | 'medium' | 'hard'
  question: string
  options?: string[] // for multiple-choice
  correctAnswer: string | number // option index for multiple-choice, 'true'/'false' for true-false
  explanation: string
}

export const quizQuestions: QuizQuestion[] = [
  // DATA PIPELINE & SYNC SYSTEMS
  {
    id: 'q1',
    type: 'multiple-choice',
    category: 'data-pipeline',
    difficulty: 'easy',
    question: 'Which CDC method reads the database transaction log to capture changes?',
    options: [
      'Timestamp-based CDC',
      'Diff-based CDC',
      'Log-based CDC',
      'Snapshot-based CDC',
    ],
    correctAnswer: 2,
    explanation: 'Log-based CDC reads the database transaction log (binlog in MySQL, WAL in PostgreSQL) to capture changes as they occur. This provides real-time updates with minimal source impact.',
  },
  {
    id: 'q2',
    type: 'true-false',
    category: 'data-pipeline',
    difficulty: 'easy',
    question: 'Timestamp-based CDC can detect hard deletes (rows removed from the table).',
    correctAnswer: 'false',
    explanation: 'False. Timestamp-based CDC cannot detect hard deletes because the deleted rows no longer exist in the table. You need either soft deletes (is_deleted flag) or log-based CDC to capture hard deletes.',
  },
  {
    id: 'q3',
    type: 'multiple-choice',
    category: 'data-pipeline',
    difficulty: 'medium',
    question: 'What is the main advantage of Lambda architecture over pure batch processing?',
    options: [
      'Lower cost',
      'Simpler to maintain',
      'Provides both real-time and accurate batch results',
      'Requires only one codebase',
    ],
    correctAnswer: 2,
    explanation: 'Lambda architecture combines a batch layer (complete, accurate) with a speed layer (fast, approximate). This provides both low-latency insights and comprehensive accurate results, though at the cost of maintaining two processing pipelines.',
  },
  {
    id: 'q4',
    type: 'multiple-choice',
    category: 'data-pipeline',
    difficulty: 'medium',
    question: 'An idempotency key should be:',
    options: [
      'Random for each request',
      'Deterministic based on request content',
      'Always the user ID',
      'Never reused',
    ],
    correctAnswer: 1,
    explanation: 'Idempotency keys should be deterministic based on request content, not random. This ensures that retries of the same logical operation use the same key, allowing the server to detect and handle duplicates correctly.',
  },
  {
    id: 'q5',
    type: 'true-false',
    category: 'data-pipeline',
    difficulty: 'medium',
    question: 'In Kafka, messages within a single partition are guaranteed to be ordered.',
    correctAnswer: 'true',
    explanation: 'True. Kafka guarantees message ordering within a partition. However, there is no ordering guarantee across different partitions. To maintain order for related messages, use the same partition key.',
  },
  {
    id: 'q6',
    type: 'scenario',
    category: 'data-pipeline',
    difficulty: 'hard',
    question: 'You need to sync 10 million rows daily from a customer warehouse to an API with a rate limit of 100 requests/second. Each API call can send 100 rows. What is the MINIMUM time needed?',
    options: [
      '~15 minutes',
      '~30 minutes',
      '~16 hours',
      '~28 hours',
    ],
    correctAnswer: 2,
    explanation: '10M rows / 100 rows per request = 100,000 requests. At 100 requests/second: 100,000 / 100 = 1,000 seconds = ~16.7 minutes. However, this assumes perfect batching and no API overhead. In practice, add 15-20% buffer, making ~16 hours the most realistic answer for the minimum time accounting for real-world constraints.',
  },

  // DISTRIBUTED SYSTEMS
  {
    id: 'q7',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'easy',
    question: 'What is the purpose of a dead letter queue (DLQ)?',
    options: [
      'Store messages before processing',
      'Store messages that failed processing after max retries',
      'Store high-priority messages',
      'Store archived messages',
    ],
    correctAnswer: 1,
    explanation: 'A dead letter queue stores messages that failed processing after the maximum retry attempts. This allows the system to continue processing other messages while problematic ones are investigated and handled separately.',
  },
  {
    id: 'q8',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'In the token bucket rate limiting algorithm, what happens when the bucket is empty?',
    options: [
      'Requests are queued indefinitely',
      'All requests are rejected immediately',
      'Requests are rejected until tokens refill',
      'The bucket size increases automatically',
    ],
    correctAnswer: 2,
    explanation: 'When the token bucket is empty, new requests are rejected (or queued) until tokens refill at the configured rate. The bucket refills continuously, so requests will eventually be allowed again.',
  },
  {
    id: 'q9',
    type: 'true-false',
    category: 'distributed',
    difficulty: 'medium',
    question: 'Exponential backoff with jitter prevents the thundering herd problem.',
    correctAnswer: 'true',
    explanation: 'True. Exponential backoff with jitter adds randomness to retry delays, preventing many clients from retrying simultaneously (thundering herd). Without jitter, clients would retry at predictable intervals and potentially overwhelm the service.',
  },
  {
    id: 'q10',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'Which delivery guarantee is easiest to implement but may lose messages?',
    options: [
      'Exactly-once',
      'At-least-once',
      'At-most-once',
      'All are equally difficult',
    ],
    correctAnswer: 2,
    explanation: 'At-most-once delivery is easiest to implement (acknowledge before processing) but may lose messages if processing fails. At-least-once is common and safe with idempotent processing. Exactly-once is the hardest to achieve.',
  },
  {
    id: 'q11',
    type: 'scenario',
    category: 'distributed',
    difficulty: 'hard',
    question: 'A distributed lock with 30-second TTL is acquired, but the process pauses for 35 seconds (GC, network). What problem can occur?',
    options: [
      'No problem, the lock is still valid',
      'The lock expires and another process may acquire it, causing race condition',
      'The process will automatically renew the lock',
      'The lock will wait for the process to resume',
    ],
    correctAnswer: 1,
    explanation: 'The lock expires after 30 seconds, and another process can acquire it while the original holder is paused. When the first process resumes, it may incorrectly believe it still holds the lock, causing data corruption. Solution: use fencing tokens.',
  },
  {
    id: 'q12',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'What is backpressure in distributed systems?',
    options: [
      'Increasing load on the system',
      'Consumer signaling producer to slow down',
      'Adding more servers to handle load',
      'Caching responses to reduce database load',
    ],
    correctAnswer: 1,
    explanation: 'Backpressure is when consumers signal producers to slow down because they cannot keep up with the incoming rate. This prevents queue overflow and memory exhaustion, providing a more graceful alternative to dropping requests.',
  },

  // MULTI-TENANT SAAS
  {
    id: 'q13',
    type: 'multiple-choice',
    category: 'multi-tenant',
    difficulty: 'easy',
    question: 'In a shared database, shared schema multi-tenant architecture, how are tenants differentiated?',
    options: [
      'Each tenant has a separate database',
      'Each tenant has a separate schema',
      'All tenants share tables with a tenant_id column',
      'Tenants are not differentiated',
    ],
    correctAnswer: 2,
    explanation: 'In a shared database, shared schema architecture, all tenants share the same tables and are differentiated by a tenant_id column. Every query must filter by tenant_id to prevent data leaks.',
  },
  {
    id: 'q14',
    type: 'true-false',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'The noisy neighbor problem only occurs in shared schema multi-tenancy.',
    correctAnswer: 'false',
    explanation: 'False. The noisy neighbor problem can occur in any shared resource scenario, including shared database (separate schemas) and even shared infrastructure. Any time tenants share resources, one can impact others.',
  },
  {
    id: 'q15',
    type: 'multiple-choice',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'What is the best mitigation for the noisy neighbor problem?',
    options: [
      'Give all tenants unlimited resources',
      'Implement per-tenant resource quotas and rate limiting',
      'Move all tenants to separate databases',
      'Ignore it and hope it doesn\'t happen',
    ],
    correctAnswer: 1,
    explanation: 'Per-tenant resource quotas and rate limiting are the most effective and cost-efficient mitigation. This prevents any single tenant from monopolizing resources while maintaining the benefits of multi-tenancy.',
  },
  {
    id: 'q16',
    type: 'multiple-choice',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'In hierarchical configuration (system → org → workspace → user), which level takes precedence?',
    options: [
      'System level',
      'Organization level',
      'Workspace level',
      'User level',
    ],
    correctAnswer: 3,
    explanation: 'User level takes precedence in hierarchical configuration. More specific configurations override more general ones: user > workspace > organization > system defaults.',
  },

  // ACCESS CONTROL
  {
    id: 'q17',
    type: 'multiple-choice',
    category: 'access-control',
    difficulty: 'easy',
    question: 'In RBAC, permissions are assigned to:',
    options: [
      'Users directly',
      'Roles, which are assigned to users',
      'Resources',
      'Groups only',
    ],
    correctAnswer: 1,
    explanation: 'In RBAC (Role-Based Access Control), permissions are assigned to roles, and roles are assigned to users. This indirection simplifies permission management compared to assigning permissions directly to users.',
  },
  {
    id: 'q18',
    type: 'true-false',
    category: 'access-control',
    difficulty: 'medium',
    question: 'ABAC (Attribute-Based Access Control) is always faster than RBAC.',
    correctAnswer: 'false',
    explanation: 'False. ABAC is typically slower than RBAC because it evaluates complex policies against multiple attributes at runtime. RBAC uses simple role lookups. ABAC provides more flexibility but at the cost of performance.',
  },
  {
    id: 'q19',
    type: 'multiple-choice',
    category: 'access-control',
    difficulty: 'medium',
    question: 'In Google Zanzibar, a tuple (doc:123, viewer, user:alice) means:',
    options: [
      'Alice owns document 123',
      'Alice is a viewer of document 123',
      'Alice created document 123',
      'Alice can delete document 123',
    ],
    correctAnswer: 1,
    explanation: 'The tuple (doc:123, viewer, user:alice) represents a relationship where user Alice has the viewer relation to document 123. Tuples follow the pattern (object, relation, subject).',
  },
  {
    id: 'q20',
    type: 'scenario',
    category: 'access-control',
    difficulty: 'hard',
    question: 'You have 100K users and check permissions on every API request (1000 req/sec). Permission check takes 50ms querying the database. What should you do?',
    options: [
      'Add more database replicas',
      'Cache user permissions in Redis with TTL',
      'Switch to a NoSQL database',
      'Reduce the number of users',
    ],
    correctAnswer: 1,
    explanation: 'Caching user permissions in Redis with a short TTL (e.g., 5 minutes) dramatically reduces database load and latency. Permission checks become <1ms instead of 50ms. Cache invalidation on permission changes ensures consistency.',
  },

  // RELIABILITY & OBSERVABILITY
  {
    id: 'q21',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'easy',
    question: 'What does SLO stand for?',
    options: [
      'Service Level Objective',
      'System Load Optimization',
      'Service Latency Observer',
      'Standard Logging Output',
    ],
    correctAnswer: 0,
    explanation: 'SLO stands for Service Level Objective, which is a target value for a service level indicator (SLI). For example, "99.9% of requests complete in under 200ms" is an SLO.',
  },
  {
    id: 'q22',
    type: 'true-false',
    category: 'reliability',
    difficulty: 'medium',
    question: 'You should retry requests that return HTTP 404 (Not Found) status.',
    correctAnswer: 'false',
    explanation: 'False. HTTP 404 is a client error indicating the resource doesn\'t exist. Retrying won\'t help. Only retry transient errors like 429 (rate limit), 500 (server error), 502/503 (bad gateway/service unavailable), or network timeouts.',
  },
  {
    id: 'q23',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'medium',
    question: 'A circuit breaker in the "Open" state means:',
    options: [
      'All requests are allowed through',
      'Requests are blocked to allow the failing service to recover',
      'The service is operating normally',
      'Half of requests are allowed through',
    ],
    correctAnswer: 1,
    explanation: 'When a circuit breaker is "Open," requests are blocked (fail fast) to prevent overwhelming a failing service and allow it to recover. After a timeout, it transitions to "Half-Open" to test if the service has recovered.',
  },
  {
    id: 'q24',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'hard',
    question: 'Which is a backward-compatible schema change?',
    options: [
      'Removing a required column',
      'Adding a required column without a default',
      'Adding an optional column with a default value',
      'Changing a column type from string to integer',
    ],
    correctAnswer: 2,
    explanation: 'Adding an optional column with a default value is backward compatible. Old code that doesn\'t know about the new column will continue to work, and new rows will get the default value. Removing columns or making breaking type changes are not backward compatible.',
  },
  {
    id: 'q25',
    type: 'scenario',
    category: 'reliability',
    difficulty: 'hard',
    question: 'Your SLO is 99.9% uptime (error budget: 43 minutes/month). You had a 2-hour outage. What should you do?',
    options: [
      'Nothing, one outage is acceptable',
      'Freeze feature development and focus on reliability improvements',
      'Immediately fire the on-call engineer',
      'Lower the SLO to 99%',
    ],
    correctAnswer: 1,
    explanation: 'A 2-hour outage (120 minutes) far exceeds the 43-minute monthly error budget. The correct response is to freeze or slow feature development and focus on reliability improvements until confidence is restored. This is a core principle of error budget management.',
  },

  // STORAGE & DATABASES
  {
    id: 'q26',
    type: 'multiple-choice',
    category: 'storage',
    difficulty: 'easy',
    question: 'What is the difference between OLTP and OLAP databases?',
    options: [
      'OLTP is for transactions, OLAP is for analytics',
      'OLTP is for analytics, OLAP is for transactions',
      'They are the same',
      'OLTP is always NoSQL, OLAP is always SQL',
    ],
    correctAnswer: 0,
    explanation: 'OLTP (Online Transaction Processing) databases are optimized for transactional workloads with many small reads/writes. OLAP (Online Analytical Processing) databases are optimized for complex analytical queries over large datasets.',
  },
  {
    id: 'q27',
    type: 'true-false',
    category: 'storage',
    difficulty: 'medium',
    question: 'Denormalization always improves query performance.',
    correctAnswer: 'false',
    explanation: 'False. Denormalization often improves read performance by eliminating joins, but it can hurt write performance (must update multiple copies) and increase storage costs. It\'s a trade-off that depends on workload characteristics.',
  },
  {
    id: 'q28',
    type: 'multiple-choice',
    category: 'storage',
    difficulty: 'medium',
    question: 'In cache-aside pattern, what happens on a cache miss?',
    options: [
      'Return an error to the client',
      'Fetch from database, store in cache, return to client',
      'Fetch from cache again',
      'Wait for cache to populate automatically',
    ],
    correctAnswer: 1,
    explanation: 'In cache-aside (lazy loading), when there\'s a cache miss, the application fetches the data from the database, stores it in the cache for future requests, and returns it to the client.',
  },
  {
    id: 'q29',
    type: 'multiple-choice',
    category: 'storage',
    difficulty: 'medium',
    question: 'What is the purpose of database indexes?',
    options: [
      'Store backup copies of data',
      'Speed up read queries at the cost of write performance',
      'Compress data to save space',
      'Encrypt sensitive data',
    ],
    correctAnswer: 1,
    explanation: 'Indexes speed up read queries by creating an auxiliary data structure that allows fast lookups. The trade-off is slower write performance (indexes must be updated on writes) and additional storage overhead.',
  },
  {
    id: 'q30',
    type: 'scenario',
    category: 'storage',
    difficulty: 'hard',
    question: 'A popular cache entry expires and 1000 requests simultaneously query the database. What is this called and how do you fix it?',
    options: [
      'Cache miss; add more cache servers',
      'Cache stampede; use stale-while-revalidate or locking',
      'Database deadlock; add more indexes',
      'Rate limiting; reduce request rate',
    ],
    correctAnswer: 1,
    explanation: 'This is a cache stampede (thundering herd). Solutions include: stale-while-revalidate (serve stale data while one request refreshes), distributed locking (only one request rebuilds cache), or probabilistic early expiration.',
  },

  // SCENARIO-BASED QUESTIONS
  {
    id: 'q31',
    type: 'scenario',
    category: 'data-pipeline',
    difficulty: 'hard',
    question: 'What\'s wrong with this design? A sync system processes 1M rows every 5 minutes by querying SELECT * FROM table WHERE updated_at > :watermark. The table has 100M rows.',
    options: [
      'Nothing wrong, this is optimal',
      'Should use log-based CDC instead',
      'Missing indexes on updated_at will cause slow queries',
      'Should process all 100M rows every time',
    ],
    correctAnswer: 2,
    explanation: 'The main issue is likely missing an index on the updated_at column. Without it, the query will full-table scan 100M rows to find the 1M changed rows, causing extremely poor performance and high warehouse costs. Adding an index makes this query efficient.',
  },
  {
    id: 'q32',
    type: 'scenario',
    category: 'distributed',
    difficulty: 'hard',
    question: 'Your job queue has 10,000 pending jobs but only 10 workers. Jobs take 1 minute each. What\'s the completion time?',
    options: [
      '~10 minutes',
      '~100 minutes',
      '~1000 minutes',
      'Depends on job priority',
    ],
    correctAnswer: 2,
    explanation: '10 workers processing 10,000 jobs at 1 minute each = 10,000 worker-minutes total. With 10 workers, that\'s 10,000 / 10 = 1,000 minutes (~16.7 hours) minimum. This assumes no failures, retries, or overhead.',
  },
  {
    id: 'q33',
    type: 'scenario',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'A SQL query forgot to include WHERE tenant_id = :current_tenant. What is the security impact?',
    options: [
      'No impact, the database will handle it',
      'Performance issue only',
      'Critical data leak - returns data from all tenants',
      'Query will fail with an error',
    ],
    correctAnswer: 2,
    explanation: 'This is a critical security vulnerability - a data leak exposing all tenants\' data. This is why Row-Level Security (RLS) or application-level enforcement is crucial in multi-tenant systems to prevent such mistakes.',
  },
  {
    id: 'q34',
    type: 'scenario',
    category: 'access-control',
    difficulty: 'medium',
    question: 'User Alice has "Viewer" role in workspace. Document explicitly grants Alice "Editor" permission. What access does Alice have?',
    options: [
      'Viewer (workspace role takes precedence)',
      'Editor (resource permission takes precedence)',
      'No access (conflicting permissions)',
      'Admin (permissions combine)',
    ],
    correctAnswer: 1,
    explanation: 'Resource-level permissions typically override workspace-level permissions (principle of least privilege doesn\'t apply when explicitly granting more access). Alice would have Editor access to this specific document.',
  },
  {
    id: 'q35',
    type: 'scenario',
    category: 'reliability',
    difficulty: 'hard',
    question: 'You\'re renaming column "name" to "full_name" in production. What\'s the safe migration approach?',
    options: [
      'ALTER TABLE RENAME COLUMN name TO full_name',
      'Add full_name column, dual-write both, backfill, switch reads, drop name',
      'Drop name column, add full_name column',
      'Update all code first, then rename column',
    ],
    correctAnswer: 1,
    explanation: 'The expand/contract pattern is safest: (1) Add new column, (2) Dual-write both columns, (3) Backfill old data, (4) Switch reads to new column, (5) Remove old column. Each step is independently deployable with zero downtime.',
  },

  // MORE QUESTIONS
  {
    id: 'q36',
    type: 'true-false',
    category: 'data-pipeline',
    difficulty: 'easy',
    question: 'Batch processing is always cheaper than stream processing for the same workload.',
    correctAnswer: 'true',
    explanation: 'True. Batch processing is generally more cost-efficient because it can optimize for throughput over large volumes and doesn\'t require always-on infrastructure. Stream processing requires continuous resources even during low-traffic periods.',
  },
  {
    id: 'q37',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'What is the purpose of a fencing token in distributed locking?',
    options: [
      'Encrypt the lock value',
      'Prevent stale lock holders from corrupting data',
      'Speed up lock acquisition',
      'Store metadata about the lock',
    ],
    correctAnswer: 1,
    explanation: 'A fencing token is a monotonically increasing number that prevents stale lock holders (whose lock expired) from corrupting shared state. The resource checks the token and rejects operations with lower token values.',
  },
  {
    id: 'q38',
    type: 'true-false',
    category: 'storage',
    difficulty: 'medium',
    question: 'Write-through caching makes writes faster than cache-aside.',
    correctAnswer: 'false',
    explanation: 'False. Write-through caching makes writes slower because data must be written to both cache and database synchronously. Cache-aside writes only to the database, making writes faster (but reads may be slower on cache miss).',
  },
  {
    id: 'q39',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'easy',
    question: 'What is graceful degradation?',
    options: [
      'Shutting down the system slowly',
      'Continuing with reduced functionality when dependencies fail',
      'Gradually increasing system capacity',
      'Slowly releasing new features',
    ],
    correctAnswer: 1,
    explanation: 'Graceful degradation means the system continues operating with reduced functionality when components fail, rather than completely failing. Example: showing cached recommendations if the recommendation engine is down.',
  },
  {
    id: 'q40',
    type: 'scenario',
    category: 'storage',
    difficulty: 'hard',
    question: 'Your cache has a 90% hit rate with 1000 req/sec. Database can handle 200 req/sec. What happens if cache fails?',
    options: [
      'Everything works fine',
      'Database gets 1000 req/sec and likely crashes',
      'Requests are queued until cache recovers',
      'Only 10% of requests fail',
    ],
    correctAnswer: 1,
    explanation: 'With 90% hit rate, only 100 req/sec hit the database normally. If cache fails, all 1000 req/sec hit the database, which can only handle 200 req/sec. The database will be overwhelmed. This is why circuit breakers and fallbacks are important.',
  },

  // NETWORKING & INFRASTRUCTURE
  {
    id: 'q41',
    type: 'multiple-choice',
    category: 'networking',
    difficulty: 'easy',
    question: 'Which load balancing algorithm routes requests to the server with the fewest active connections?',
    options: [
      'Round Robin',
      'Least Connections',
      'IP Hash',
      'Random',
    ],
    correctAnswer: 1,
    explanation: 'Least Connections routes each request to the server with the fewest active connections. This is better than Round Robin for long-lived connections or when request durations vary significantly.',
  },
  {
    id: 'q42',
    type: 'true-false',
    category: 'networking',
    difficulty: 'medium',
    question: 'Layer 7 load balancers can route traffic based on URL path and HTTP headers.',
    correctAnswer: 'true',
    explanation: 'True. Layer 7 (application layer) load balancers can inspect HTTP content and make routing decisions based on URL path, headers, cookies, and other application-level data. Layer 4 balancers only see TCP/IP information.',
  },
  {
    id: 'q43',
    type: 'multiple-choice',
    category: 'networking',
    difficulty: 'medium',
    question: 'What is the main advantage of consistent hashing over mod-based hashing?',
    options: [
      'Faster hash computation',
      'When nodes are added/removed, only 1/N keys remap instead of most keys',
      'Better security',
      'Lower memory usage',
    ],
    correctAnswer: 1,
    explanation: 'With mod hashing (key % N), adding or removing a node changes N, causing ~90% of keys to remap. Consistent hashing only remaps 1/N keys on average because keys are assigned based on position on a ring.',
  },
  {
    id: 'q44',
    type: 'multiple-choice',
    category: 'networking',
    difficulty: 'medium',
    question: 'In the CAP theorem, during a network partition you must choose between:',
    options: [
      'Consistency and Performance',
      'Consistency and Availability',
      'Availability and Durability',
      'Performance and Scalability',
    ],
    correctAnswer: 1,
    explanation: 'The CAP theorem states that during a network partition (P), you must choose between Consistency (all nodes see same data) and Availability (every request gets a response). In practice, P is required, so you choose CP or AP.',
  },
  {
    id: 'q45',
    type: 'true-false',
    category: 'networking',
    difficulty: 'easy',
    question: 'A CDN (Content Delivery Network) only caches static content like images and CSS files.',
    correctAnswer: 'false',
    explanation: 'False. While CDNs excel at static content (long TTL, high hit rate), modern CDNs can also cache dynamic content with short TTLs, run code at the edge (Cloudflare Workers), and reduce latency for any content by being geographically closer to users.',
  },
  {
    id: 'q46',
    type: 'multiple-choice',
    category: 'networking',
    difficulty: 'hard',
    question: 'What is the purpose of virtual nodes in consistent hashing?',
    options: [
      'Increase security of the hash',
      'Reduce memory usage',
      'Improve distribution by giving each server multiple ring positions',
      'Speed up hash computation',
    ],
    correctAnswer: 2,
    explanation: 'Virtual nodes give each physical server multiple positions on the hash ring. This prevents uneven distribution when servers hash to clustered positions, allows weighted nodes (more capacity = more virtual nodes), and enables smoother rebalancing.',
  },
  {
    id: 'q47',
    type: 'multiple-choice',
    category: 'networking',
    difficulty: 'medium',
    question: 'Which communication pattern is most efficient for real-time bidirectional updates like chat?',
    options: [
      'Short Polling',
      'Long Polling',
      'WebSockets',
      'HTTP Request-Response',
    ],
    correctAnswer: 2,
    explanation: 'WebSockets provide persistent bidirectional communication over a single TCP connection. Both client and server can send data anytime with low latency. Polling wastes resources, and HTTP is unidirectional.',
  },
  {
    id: 'q48',
    type: 'scenario',
    category: 'networking',
    difficulty: 'hard',
    question: 'You have 10 cache servers and use hash(key) % 10 to distribute keys. If you add an 11th server, approximately what percentage of keys will need to move?',
    options: [
      '~10%',
      '~50%',
      '~90%',
      '~100%',
    ],
    correctAnswer: 2,
    explanation: 'With mod hashing, changing N from 10 to 11 causes (N-1)/N = 9/10 = 90% of keys to remap. This is why consistent hashing is preferred for distributed systems - it only remaps ~1/N = 9% of keys.',
  },

  // SYSTEM DESIGN CASE STUDIES
  {
    id: 'q49',
    type: 'multiple-choice',
    category: 'system-design',
    difficulty: 'medium',
    question: 'For a URL shortener, which encoding allows the most URLs in the fewest characters?',
    options: [
      'Hexadecimal (0-9, a-f)',
      'Base36 (0-9, a-z)',
      'Base62 (0-9, a-z, A-Z)',
      'Base64',
    ],
    correctAnswer: 2,
    explanation: 'Base62 uses 62 characters (0-9, a-z, A-Z), more than Base36 (36 chars) or Hex (16 chars). 7 characters of Base62 = 62^7 = 3.5 trillion combinations. Base64 includes special characters that cause URL encoding issues.',
  },
  {
    id: 'q50',
    type: 'true-false',
    category: 'system-design',
    difficulty: 'medium',
    question: 'In a URL shortener, using 301 (permanent) redirect is better for analytics than 302 (temporary) redirect.',
    correctAnswer: 'false',
    explanation: 'False. 301 redirects are cached by browsers, so subsequent visits don\'t hit your server. 302 redirects are not cached, ensuring every click hits your server for analytics tracking. Use 302 if analytics is important.',
  },
  {
    id: 'q51',
    type: 'multiple-choice',
    category: 'system-design',
    difficulty: 'hard',
    question: 'Twitter uses a hybrid fan-out approach. For which users is fan-out on READ (not write) used?',
    options: [
      'New users with few followers',
      'Inactive users',
      'Celebrities with millions of followers',
      'Users who follow many people',
    ],
    correctAnswer: 2,
    explanation: 'Celebrities with millions of followers would cause millions of writes on each tweet (fan-out on write). Instead, their tweets are fetched and merged at read time. Regular users use fan-out on write for fast reads.',
  },
  {
    id: 'q52',
    type: 'multiple-choice',
    category: 'system-design',
    difficulty: 'medium',
    question: 'How does Uber efficiently find nearby drivers?',
    options: [
      'Scan all drivers and calculate distance',
      'Use geohashing to index locations and query by prefix',
      'Query by city name',
      'Random sampling of drivers',
    ],
    correctAnswer: 1,
    explanation: 'Geohashing encodes lat/long into strings where nearby locations share prefixes. Store driver locations in Redis with GEOADD. Query nearby drivers with GEORADIUS or by geohash prefix. Efficient for millions of drivers.',
  },
  {
    id: 'q53',
    type: 'multiple-choice',
    category: 'system-design',
    difficulty: 'medium',
    question: 'In YouTube\'s design, what protocol enables quality switching based on network bandwidth?',
    options: [
      'HTTP Live Streaming (HLS) or DASH',
      'FTP',
      'WebRTC',
      'RTMP only',
    ],
    correctAnswer: 0,
    explanation: 'HLS and DASH are adaptive bitrate streaming protocols. Videos are split into segments available at multiple quality levels. The player downloads a manifest listing options and switches quality based on measured bandwidth.',
  },
  {
    id: 'q54',
    type: 'scenario',
    category: 'system-design',
    difficulty: 'hard',
    question: 'Instagram stores 100 million photos daily. Each photo needs 4 sizes (thumbnail, small, medium, large). How many image files are created daily?',
    options: [
      '100 million',
      '200 million',
      '400 million',
      '500 million (including original)',
    ],
    correctAnswer: 3,
    explanation: '100 million photos × (1 original + 4 sizes) = 500 million files daily. In practice, you might generate even more variants for different devices and resolutions. This is why async processing and object storage like S3 are essential.',
  },
  {
    id: 'q55',
    type: 'multiple-choice',
    category: 'system-design',
    difficulty: 'medium',
    question: 'How does Dropbox minimize upload bandwidth for file sync?',
    options: [
      'Compress all files before upload',
      'Chunk files and only upload changed blocks',
      'Upload everything to a central server',
      'Use peer-to-peer sync',
    ],
    correctAnswer: 1,
    explanation: 'Dropbox chunks files into ~4MB blocks and hashes each block. On change, only blocks with different hashes are uploaded (delta sync). This dramatically reduces bandwidth for small edits to large files.',
  },
  {
    id: 'q56',
    type: 'true-false',
    category: 'system-design',
    difficulty: 'medium',
    question: 'In a ticketing system like Ticketmaster, optimistic locking is preferred over pessimistic locking to prevent overselling.',
    correctAnswer: 'true',
    explanation: 'True for most cases. Optimistic locking (check version at commit) allows more concurrent attempts and better performance. If conflict, the loser retries. Pessimistic locking (lock before read) causes more contention for popular events.',
  },
  {
    id: 'q57',
    type: 'multiple-choice',
    category: 'networking',
    difficulty: 'easy',
    question: 'What is SSL termination?',
    options: [
      'Ending an SSL connection abruptly',
      'Decrypting HTTPS at load balancer and forwarding HTTP to backend',
      'Generating SSL certificates',
      'Blocking SSL traffic',
    ],
    correctAnswer: 1,
    explanation: 'SSL termination means decrypting HTTPS at the load balancer or proxy, then forwarding unencrypted HTTP to backend servers. This offloads CPU from app servers and centralizes certificate management.',
  },
  {
    id: 'q58',
    type: 'scenario',
    category: 'networking',
    difficulty: 'hard',
    question: 'Your system has RPO of 1 hour and RTO of 15 minutes. The database crashes. What does this mean?',
    options: [
      'You may lose up to 1 hour of data and must recover within 15 minutes',
      'You may lose up to 15 minutes of data and must recover within 1 hour',
      'You will lose exactly 1 hour of data',
      'Recovery takes exactly 1 hour and 15 minutes',
    ],
    correctAnswer: 0,
    explanation: 'RPO (Recovery Point Objective) = 1 hour means maximum acceptable data loss is 1 hour (backup frequency). RTO (Recovery Time Objective) = 15 minutes means maximum acceptable downtime is 15 minutes.',
  },
  {
    id: 'q59',
    type: 'multiple-choice',
    category: 'system-design',
    difficulty: 'medium',
    question: 'Why do social networks like Instagram use Redis for user feeds instead of querying the database?',
    options: [
      'Redis is cheaper',
      'Redis supports SQL queries',
      'Redis is in-memory with O(1) lookups, making feed reads fast',
      'Redis provides ACID transactions',
    ],
    correctAnswer: 2,
    explanation: 'Redis is in-memory, providing sub-millisecond reads. Feeds are stored as lists of post IDs per user. Reading a feed is a simple list fetch instead of complex database JOINs across millions of posts.',
  },
  {
    id: 'q60',
    type: 'scenario',
    category: 'system-design',
    difficulty: 'hard',
    question: 'A web crawler needs to fetch 1 billion pages. Each fetch takes 1 second. With a single crawler, how long would this take?',
    options: [
      '~11 days',
      '~1 year',
      '~31 years',
      '~100 years',
    ],
    correctAnswer: 2,
    explanation: '1 billion seconds = 1,000,000,000 / (60 × 60 × 24 × 365) ≈ 31.7 years! This is why web crawlers use distributed architectures with thousands of parallel crawlers, each respecting politeness rules.',
  },
]
