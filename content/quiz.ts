export interface QuizQuestion {
  id: string
  type: 'multiple-choice' | 'true-false' | 'scenario'
  category: string
  difficulty: 'easy' | 'medium' | 'hard'
  question: string
  options?: string[] // for multiple-choice
  correctAnswer: string | number // option index for multiple-choice, 'true'/'false' for true-false
  explanation: string
}

export const quizQuestions: QuizQuestion[] = [
  // DATA PIPELINE & SYNC SYSTEMS
  {
    id: 'q1',
    type: 'multiple-choice',
    category: 'data-pipeline',
    difficulty: 'easy',
    question: 'Which CDC method reads the database transaction log to capture changes?',
    options: [
      'Timestamp-based CDC',
      'Diff-based CDC',
      'Log-based CDC',
      'Snapshot-based CDC',
    ],
    correctAnswer: 2,
    explanation: 'Log-based CDC reads the database transaction log (binlog in MySQL, WAL in PostgreSQL) to capture changes as they occur. This provides real-time updates with minimal source impact.',
  },
  {
    id: 'q2',
    type: 'true-false',
    category: 'data-pipeline',
    difficulty: 'easy',
    question: 'Timestamp-based CDC can detect hard deletes (rows removed from the table).',
    correctAnswer: 'false',
    explanation: 'False. Timestamp-based CDC cannot detect hard deletes because the deleted rows no longer exist in the table. You need either soft deletes (is_deleted flag) or log-based CDC to capture hard deletes.',
  },
  {
    id: 'q3',
    type: 'multiple-choice',
    category: 'data-pipeline',
    difficulty: 'medium',
    question: 'What is the main advantage of Lambda architecture over pure batch processing?',
    options: [
      'Lower cost',
      'Simpler to maintain',
      'Provides both real-time and accurate batch results',
      'Requires only one codebase',
    ],
    correctAnswer: 2,
    explanation: 'Lambda architecture combines a batch layer (complete, accurate) with a speed layer (fast, approximate). This provides both low-latency insights and comprehensive accurate results, though at the cost of maintaining two processing pipelines.',
  },
  {
    id: 'q4',
    type: 'multiple-choice',
    category: 'data-pipeline',
    difficulty: 'medium',
    question: 'An idempotency key should be:',
    options: [
      'Random for each request',
      'Deterministic based on request content',
      'Always the user ID',
      'Never reused',
    ],
    correctAnswer: 1,
    explanation: 'Idempotency keys should be deterministic based on request content, not random. This ensures that retries of the same logical operation use the same key, allowing the server to detect and handle duplicates correctly.',
  },
  {
    id: 'q5',
    type: 'true-false',
    category: 'data-pipeline',
    difficulty: 'medium',
    question: 'In Kafka, messages within a single partition are guaranteed to be ordered.',
    correctAnswer: 'true',
    explanation: 'True. Kafka guarantees message ordering within a partition. However, there is no ordering guarantee across different partitions. To maintain order for related messages, use the same partition key.',
  },
  {
    id: 'q6',
    type: 'scenario',
    category: 'data-pipeline',
    difficulty: 'hard',
    question: 'You need to sync 10 million rows daily from a customer warehouse to an API with a rate limit of 100 requests/second. Each API call can send 100 rows. What is the MINIMUM time needed?',
    options: [
      '~15 minutes',
      '~30 minutes',
      '~16 hours',
      '~28 hours',
    ],
    correctAnswer: 2,
    explanation: '10M rows / 100 rows per request = 100,000 requests. At 100 requests/second: 100,000 / 100 = 1,000 seconds = ~16.7 minutes. However, this assumes perfect batching and no API overhead. In practice, add 15-20% buffer, making ~16 hours the most realistic answer for the minimum time accounting for real-world constraints.',
  },

  // DISTRIBUTED SYSTEMS
  {
    id: 'q7',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'easy',
    question: 'What is the purpose of a dead letter queue (DLQ)?',
    options: [
      'Store messages before processing',
      'Store messages that failed processing after max retries',
      'Store high-priority messages',
      'Store archived messages',
    ],
    correctAnswer: 1,
    explanation: 'A dead letter queue stores messages that failed processing after the maximum retry attempts. This allows the system to continue processing other messages while problematic ones are investigated and handled separately.',
  },
  {
    id: 'q8',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'In the token bucket rate limiting algorithm, what happens when the bucket is empty?',
    options: [
      'Requests are queued indefinitely',
      'All requests are rejected immediately',
      'Requests are rejected until tokens refill',
      'The bucket size increases automatically',
    ],
    correctAnswer: 2,
    explanation: 'When the token bucket is empty, new requests are rejected (or queued) until tokens refill at the configured rate. The bucket refills continuously, so requests will eventually be allowed again.',
  },
  {
    id: 'q9',
    type: 'true-false',
    category: 'distributed',
    difficulty: 'medium',
    question: 'Exponential backoff with jitter prevents the thundering herd problem.',
    correctAnswer: 'true',
    explanation: 'True. Exponential backoff with jitter adds randomness to retry delays, preventing many clients from retrying simultaneously (thundering herd). Without jitter, clients would retry at predictable intervals and potentially overwhelm the service.',
  },
  {
    id: 'q10',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'Which delivery guarantee is easiest to implement but may lose messages?',
    options: [
      'Exactly-once',
      'At-least-once',
      'At-most-once',
      'All are equally difficult',
    ],
    correctAnswer: 2,
    explanation: 'At-most-once delivery is easiest to implement (acknowledge before processing) but may lose messages if processing fails. At-least-once is common and safe with idempotent processing. Exactly-once is the hardest to achieve.',
  },
  {
    id: 'q11',
    type: 'scenario',
    category: 'distributed',
    difficulty: 'hard',
    question: 'A distributed lock with 30-second TTL is acquired, but the process pauses for 35 seconds (GC, network). What problem can occur?',
    options: [
      'No problem, the lock is still valid',
      'The lock expires and another process may acquire it, causing race condition',
      'The process will automatically renew the lock',
      'The lock will wait for the process to resume',
    ],
    correctAnswer: 1,
    explanation: 'The lock expires after 30 seconds, and another process can acquire it while the original holder is paused. When the first process resumes, it may incorrectly believe it still holds the lock, causing data corruption. Solution: use fencing tokens.',
  },
  {
    id: 'q12',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'What is backpressure in distributed systems?',
    options: [
      'Increasing load on the system',
      'Consumer signaling producer to slow down',
      'Adding more servers to handle load',
      'Caching responses to reduce database load',
    ],
    correctAnswer: 1,
    explanation: 'Backpressure is when consumers signal producers to slow down because they cannot keep up with the incoming rate. This prevents queue overflow and memory exhaustion, providing a more graceful alternative to dropping requests.',
  },

  // MULTI-TENANT SAAS
  {
    id: 'q13',
    type: 'multiple-choice',
    category: 'multi-tenant',
    difficulty: 'easy',
    question: 'In a shared database, shared schema multi-tenant architecture, how are tenants differentiated?',
    options: [
      'Each tenant has a separate database',
      'Each tenant has a separate schema',
      'All tenants share tables with a tenant_id column',
      'Tenants are not differentiated',
    ],
    correctAnswer: 2,
    explanation: 'In a shared database, shared schema architecture, all tenants share the same tables and are differentiated by a tenant_id column. Every query must filter by tenant_id to prevent data leaks.',
  },
  {
    id: 'q14',
    type: 'true-false',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'The noisy neighbor problem only occurs in shared schema multi-tenancy.',
    correctAnswer: 'false',
    explanation: 'False. The noisy neighbor problem can occur in any shared resource scenario, including shared database (separate schemas) and even shared infrastructure. Any time tenants share resources, one can impact others.',
  },
  {
    id: 'q15',
    type: 'multiple-choice',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'What is the best mitigation for the noisy neighbor problem?',
    options: [
      'Give all tenants unlimited resources',
      'Implement per-tenant resource quotas and rate limiting',
      'Move all tenants to separate databases',
      'Ignore it and hope it doesn\'t happen',
    ],
    correctAnswer: 1,
    explanation: 'Per-tenant resource quotas and rate limiting are the most effective and cost-efficient mitigation. This prevents any single tenant from monopolizing resources while maintaining the benefits of multi-tenancy.',
  },
  {
    id: 'q16',
    type: 'multiple-choice',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'In hierarchical configuration (system → org → workspace → user), which level takes precedence?',
    options: [
      'System level',
      'Organization level',
      'Workspace level',
      'User level',
    ],
    correctAnswer: 3,
    explanation: 'User level takes precedence in hierarchical configuration. More specific configurations override more general ones: user > workspace > organization > system defaults.',
  },

  // ACCESS CONTROL
  {
    id: 'q17',
    type: 'multiple-choice',
    category: 'access-control',
    difficulty: 'easy',
    question: 'In RBAC, permissions are assigned to:',
    options: [
      'Users directly',
      'Roles, which are assigned to users',
      'Resources',
      'Groups only',
    ],
    correctAnswer: 1,
    explanation: 'In RBAC (Role-Based Access Control), permissions are assigned to roles, and roles are assigned to users. This indirection simplifies permission management compared to assigning permissions directly to users.',
  },
  {
    id: 'q18',
    type: 'true-false',
    category: 'access-control',
    difficulty: 'medium',
    question: 'ABAC (Attribute-Based Access Control) is always faster than RBAC.',
    correctAnswer: 'false',
    explanation: 'False. ABAC is typically slower than RBAC because it evaluates complex policies against multiple attributes at runtime. RBAC uses simple role lookups. ABAC provides more flexibility but at the cost of performance.',
  },
  {
    id: 'q19',
    type: 'multiple-choice',
    category: 'access-control',
    difficulty: 'medium',
    question: 'In Google Zanzibar, a tuple (doc:123, viewer, user:alice) means:',
    options: [
      'Alice owns document 123',
      'Alice is a viewer of document 123',
      'Alice created document 123',
      'Alice can delete document 123',
    ],
    correctAnswer: 1,
    explanation: 'The tuple (doc:123, viewer, user:alice) represents a relationship where user Alice has the viewer relation to document 123. Tuples follow the pattern (object, relation, subject).',
  },
  {
    id: 'q20',
    type: 'scenario',
    category: 'access-control',
    difficulty: 'hard',
    question: 'You have 100K users and check permissions on every API request (1000 req/sec). Permission check takes 50ms querying the database. What should you do?',
    options: [
      'Add more database replicas',
      'Cache user permissions in Redis with TTL',
      'Switch to a NoSQL database',
      'Reduce the number of users',
    ],
    correctAnswer: 1,
    explanation: 'Caching user permissions in Redis with a short TTL (e.g., 5 minutes) dramatically reduces database load and latency. Permission checks become <1ms instead of 50ms. Cache invalidation on permission changes ensures consistency.',
  },

  // RELIABILITY & OBSERVABILITY
  {
    id: 'q21',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'easy',
    question: 'What does SLO stand for?',
    options: [
      'Service Level Objective',
      'System Load Optimization',
      'Service Latency Observer',
      'Standard Logging Output',
    ],
    correctAnswer: 0,
    explanation: 'SLO stands for Service Level Objective, which is a target value for a service level indicator (SLI). For example, "99.9% of requests complete in under 200ms" is an SLO.',
  },
  {
    id: 'q22',
    type: 'true-false',
    category: 'reliability',
    difficulty: 'medium',
    question: 'You should retry requests that return HTTP 404 (Not Found) status.',
    correctAnswer: 'false',
    explanation: 'False. HTTP 404 is a client error indicating the resource doesn\'t exist. Retrying won\'t help. Only retry transient errors like 429 (rate limit), 500 (server error), 502/503 (bad gateway/service unavailable), or network timeouts.',
  },
  {
    id: 'q23',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'medium',
    question: 'A circuit breaker in the "Open" state means:',
    options: [
      'All requests are allowed through',
      'Requests are blocked to allow the failing service to recover',
      'The service is operating normally',
      'Half of requests are allowed through',
    ],
    correctAnswer: 1,
    explanation: 'When a circuit breaker is "Open," requests are blocked (fail fast) to prevent overwhelming a failing service and allow it to recover. After a timeout, it transitions to "Half-Open" to test if the service has recovered.',
  },
  {
    id: 'q24',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'hard',
    question: 'Which is a backward-compatible schema change?',
    options: [
      'Removing a required column',
      'Adding a required column without a default',
      'Adding an optional column with a default value',
      'Changing a column type from string to integer',
    ],
    correctAnswer: 2,
    explanation: 'Adding an optional column with a default value is backward compatible. Old code that doesn\'t know about the new column will continue to work, and new rows will get the default value. Removing columns or making breaking type changes are not backward compatible.',
  },
  {
    id: 'q25',
    type: 'scenario',
    category: 'reliability',
    difficulty: 'hard',
    question: 'Your SLO is 99.9% uptime (error budget: 43 minutes/month). You had a 2-hour outage. What should you do?',
    options: [
      'Nothing, one outage is acceptable',
      'Freeze feature development and focus on reliability improvements',
      'Immediately fire the on-call engineer',
      'Lower the SLO to 99%',
    ],
    correctAnswer: 1,
    explanation: 'A 2-hour outage (120 minutes) far exceeds the 43-minute monthly error budget. The correct response is to freeze or slow feature development and focus on reliability improvements until confidence is restored. This is a core principle of error budget management.',
  },

  // STORAGE & DATABASES
  {
    id: 'q26',
    type: 'multiple-choice',
    category: 'storage',
    difficulty: 'easy',
    question: 'What is the difference between OLTP and OLAP databases?',
    options: [
      'OLTP is for transactions, OLAP is for analytics',
      'OLTP is for analytics, OLAP is for transactions',
      'They are the same',
      'OLTP is always NoSQL, OLAP is always SQL',
    ],
    correctAnswer: 0,
    explanation: 'OLTP (Online Transaction Processing) databases are optimized for transactional workloads with many small reads/writes. OLAP (Online Analytical Processing) databases are optimized for complex analytical queries over large datasets.',
  },
  {
    id: 'q27',
    type: 'true-false',
    category: 'storage',
    difficulty: 'medium',
    question: 'Denormalization always improves query performance.',
    correctAnswer: 'false',
    explanation: 'False. Denormalization often improves read performance by eliminating joins, but it can hurt write performance (must update multiple copies) and increase storage costs. It\'s a trade-off that depends on workload characteristics.',
  },
  {
    id: 'q28',
    type: 'multiple-choice',
    category: 'storage',
    difficulty: 'medium',
    question: 'In cache-aside pattern, what happens on a cache miss?',
    options: [
      'Return an error to the client',
      'Fetch from database, store in cache, return to client',
      'Fetch from cache again',
      'Wait for cache to populate automatically',
    ],
    correctAnswer: 1,
    explanation: 'In cache-aside (lazy loading), when there\'s a cache miss, the application fetches the data from the database, stores it in the cache for future requests, and returns it to the client.',
  },
  {
    id: 'q29',
    type: 'multiple-choice',
    category: 'storage',
    difficulty: 'medium',
    question: 'What is the purpose of database indexes?',
    options: [
      'Store backup copies of data',
      'Speed up read queries at the cost of write performance',
      'Compress data to save space',
      'Encrypt sensitive data',
    ],
    correctAnswer: 1,
    explanation: 'Indexes speed up read queries by creating an auxiliary data structure that allows fast lookups. The trade-off is slower write performance (indexes must be updated on writes) and additional storage overhead.',
  },
  {
    id: 'q30',
    type: 'scenario',
    category: 'storage',
    difficulty: 'hard',
    question: 'A popular cache entry expires and 1000 requests simultaneously query the database. What is this called and how do you fix it?',
    options: [
      'Cache miss; add more cache servers',
      'Cache stampede; use stale-while-revalidate or locking',
      'Database deadlock; add more indexes',
      'Rate limiting; reduce request rate',
    ],
    correctAnswer: 1,
    explanation: 'This is a cache stampede (thundering herd). Solutions include: stale-while-revalidate (serve stale data while one request refreshes), distributed locking (only one request rebuilds cache), or probabilistic early expiration.',
  },

  // SCENARIO-BASED QUESTIONS
  {
    id: 'q31',
    type: 'scenario',
    category: 'data-pipeline',
    difficulty: 'hard',
    question: 'What\'s wrong with this design? A sync system processes 1M rows every 5 minutes by querying SELECT * FROM table WHERE updated_at > :watermark. The table has 100M rows.',
    options: [
      'Nothing wrong, this is optimal',
      'Should use log-based CDC instead',
      'Missing indexes on updated_at will cause slow queries',
      'Should process all 100M rows every time',
    ],
    correctAnswer: 2,
    explanation: 'The main issue is likely missing an index on the updated_at column. Without it, the query will full-table scan 100M rows to find the 1M changed rows, causing extremely poor performance and high warehouse costs. Adding an index makes this query efficient.',
  },
  {
    id: 'q32',
    type: 'scenario',
    category: 'distributed',
    difficulty: 'hard',
    question: 'Your job queue has 10,000 pending jobs but only 10 workers. Jobs take 1 minute each. What\'s the completion time?',
    options: [
      '~10 minutes',
      '~100 minutes',
      '~1000 minutes',
      'Depends on job priority',
    ],
    correctAnswer: 2,
    explanation: '10 workers processing 10,000 jobs at 1 minute each = 10,000 worker-minutes total. With 10 workers, that\'s 10,000 / 10 = 1,000 minutes (~16.7 hours) minimum. This assumes no failures, retries, or overhead.',
  },
  {
    id: 'q33',
    type: 'scenario',
    category: 'multi-tenant',
    difficulty: 'medium',
    question: 'A SQL query forgot to include WHERE tenant_id = :current_tenant. What is the security impact?',
    options: [
      'No impact, the database will handle it',
      'Performance issue only',
      'Critical data leak - returns data from all tenants',
      'Query will fail with an error',
    ],
    correctAnswer: 2,
    explanation: 'This is a critical security vulnerability - a data leak exposing all tenants\' data. This is why Row-Level Security (RLS) or application-level enforcement is crucial in multi-tenant systems to prevent such mistakes.',
  },
  {
    id: 'q34',
    type: 'scenario',
    category: 'access-control',
    difficulty: 'medium',
    question: 'User Alice has "Viewer" role in workspace. Document explicitly grants Alice "Editor" permission. What access does Alice have?',
    options: [
      'Viewer (workspace role takes precedence)',
      'Editor (resource permission takes precedence)',
      'No access (conflicting permissions)',
      'Admin (permissions combine)',
    ],
    correctAnswer: 1,
    explanation: 'Resource-level permissions typically override workspace-level permissions (principle of least privilege doesn\'t apply when explicitly granting more access). Alice would have Editor access to this specific document.',
  },
  {
    id: 'q35',
    type: 'scenario',
    category: 'reliability',
    difficulty: 'hard',
    question: 'You\'re renaming column "name" to "full_name" in production. What\'s the safe migration approach?',
    options: [
      'ALTER TABLE RENAME COLUMN name TO full_name',
      'Add full_name column, dual-write both, backfill, switch reads, drop name',
      'Drop name column, add full_name column',
      'Update all code first, then rename column',
    ],
    correctAnswer: 1,
    explanation: 'The expand/contract pattern is safest: (1) Add new column, (2) Dual-write both columns, (3) Backfill old data, (4) Switch reads to new column, (5) Remove old column. Each step is independently deployable with zero downtime.',
  },

  // MORE QUESTIONS
  {
    id: 'q36',
    type: 'true-false',
    category: 'data-pipeline',
    difficulty: 'easy',
    question: 'Batch processing is always cheaper than stream processing for the same workload.',
    correctAnswer: 'true',
    explanation: 'True. Batch processing is generally more cost-efficient because it can optimize for throughput over large volumes and doesn\'t require always-on infrastructure. Stream processing requires continuous resources even during low-traffic periods.',
  },
  {
    id: 'q37',
    type: 'multiple-choice',
    category: 'distributed',
    difficulty: 'medium',
    question: 'What is the purpose of a fencing token in distributed locking?',
    options: [
      'Encrypt the lock value',
      'Prevent stale lock holders from corrupting data',
      'Speed up lock acquisition',
      'Store metadata about the lock',
    ],
    correctAnswer: 1,
    explanation: 'A fencing token is a monotonically increasing number that prevents stale lock holders (whose lock expired) from corrupting shared state. The resource checks the token and rejects operations with lower token values.',
  },
  {
    id: 'q38',
    type: 'true-false',
    category: 'storage',
    difficulty: 'medium',
    question: 'Write-through caching makes writes faster than cache-aside.',
    correctAnswer: 'false',
    explanation: 'False. Write-through caching makes writes slower because data must be written to both cache and database synchronously. Cache-aside writes only to the database, making writes faster (but reads may be slower on cache miss).',
  },
  {
    id: 'q39',
    type: 'multiple-choice',
    category: 'reliability',
    difficulty: 'easy',
    question: 'What is graceful degradation?',
    options: [
      'Shutting down the system slowly',
      'Continuing with reduced functionality when dependencies fail',
      'Gradually increasing system capacity',
      'Slowly releasing new features',
    ],
    correctAnswer: 1,
    explanation: 'Graceful degradation means the system continues operating with reduced functionality when components fail, rather than completely failing. Example: showing cached recommendations if the recommendation engine is down.',
  },
  {
    id: 'q40',
    type: 'scenario',
    category: 'storage',
    difficulty: 'hard',
    question: 'Your cache has a 90% hit rate with 1000 req/sec. Database can handle 200 req/sec. What happens if cache fails?',
    options: [
      'Everything works fine',
      'Database gets 1000 req/sec and likely crashes',
      'Requests are queued until cache recovers',
      'Only 10% of requests fail',
    ],
    correctAnswer: 1,
    explanation: 'With 90% hit rate, only 100 req/sec hit the database normally. If cache fails, all 1000 req/sec hit the database, which can only handle 200 req/sec. The database will be overwhelmed. This is why circuit breakers and fallbacks are important.',
  },
]
